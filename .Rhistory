summary(lmmodel)
bmodel <- lm(prijs ~ plaats + type + woonoppervlakte + perceeloppervlakte + kamers + energielabel + date_diff + leeftijd,
data = na.omit(traindata))
backlmmodel <- step(bmodel, direction = "backward")
minmodel <- lm(prijs ~ 1, data = na.omit(traindata))
forlmmodel <- step(minmodel, direction = "forward",
scope = (~ plaats + type + woonoppervlakte + perceeloppervlakte + kamers + energielabel + date_diff + leeftijd),
trace = 0)
summary(forlmmodel)
testdata$predictie <- predict(lmmodel, newdata = testdata)
summary(testdata$predictie)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
install.packages("AppliedPredictiveModeling")
install.packages("caret")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
View(concrete)
View(mixtures)
data(concrete)
data(concrete)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
plot(mixtures$CompressiveStrength)
library(Hmisc)
cutAge <- cut2(mixtures$Age, g=5)
qplot(mixtures$CompressiveStrength, fill = cutAge)
qplot(cutage, mixtures$CompressiveStrength, fill = cutAge)
qplot(cutAge, mixtures$CompressiveStrength, fill = cutAge)
qplot(cutAge, mixtures$CompressiveStrength, fill = cutAge, geom = "boxplot")
cutAge <- cut2(mixtures$Age, g=4)
cutAge <- cut2(mixtures$Age, g=4)
qplot(cutAge, mixtures$CompressiveStrength, fill = cutAge, geom = "boxplot")
cutAge <- cut2(mixtures$Age, g=3)
cutAge <- cut2(mixtures$Age, g=4)
cutAge <- cut2(mixtures$Age, g=5)
cutAge <- cut2(mixtures$Age, g=4)
rm(cutAge)
training$CompressiveStrength2 <- cut2(training$CompressiveStrength, g = 4)
qplot(data = training, CompressiveStrength2, age, fill = CompressiveStrength2)
qplot(data = training, CompressiveStrength2, Age, fill = CompressiveStrength2)
qplot(data = training, CompressiveStrength2, Age, color = CompressiveStrength2)
qplot(data = training, CompressiveStrength2, FlyAsh, color = CompressiveStrength2)
cor(training$CompressiveStrength, training$FlyAsh)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
plot(data = training, SuperPlasticizer)
View(training)
plot(data = training, SuperPlasticizer)
plot(SuperPlasticizer, data = training)
plot(training$SuperPlasticizer, data = training)
ggplot(data = training, mapping = aes(SuperPlasticizer))+
geom_histogram()
View(training)
ggplot(data = training, mapping = aes(x=SuperPlasticizer))+
geom_histogram()
library(tidyverse)
ggplot(data = training, mapping = aes(x=SuperPlasticizer))+
geom_histogram()
ggplot(data = concrete, mapping = aes(x=SuperPlasticizer))+
geom_histogram()
ggplot(data = concrete, mapping = aes(x=Age))+
geom_histogram()
str(training)
ggplot(data = concrete, mapping = aes(x=Superplasticizer))+
geom_histogram()
ggplot(data = concrete, mapping = aes(x=Superplasticizer))+
geom_histogram(bins = 40)
ggplot(data = concrete, mapping = aes(x=log(Superplasticizer)))+
geom_histogram(bins = 40)
View(training)
ggplot(data = concrete, mapping = aes(x=log(Superplasticizer + 1)))+
geom_histogram(bins = 40)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
preProc <- preProcess(predictors, method = "pca", pcaComp = 10)
preProc$dim
preProc$pcaComp
preProc
preProc <- preProcess(predictors, method = "pca", thresh = 0.9)
preProc
View(training)
head(adData)
head(predictors)
str(adData)
trainingIL <- training[,grep("^IL", names(training))]
procTrain <- preProcess(trainingIL, method = "pca", thresh = 0.9 )
procTrain
install.packages("ISLR")
library(ISLR)
data(Wage)
head(Wage)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.7, list = F)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
inTrain
modFit <- train(Case ~., method="rpart", data = training)
print(modFit$finalModel)
set.seed(125)
modFit <- train(Case ~., method="rpart", data = training)
print(modFit$finalModel)
install.packages("rattle")
library(rattle)
library("rattle", lib.loc="~/R/win-library/3.4")
fancyRpartPlot(modFit$finalModel)
set.seed(125)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.7, list = F)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
modFit <- train(Case ~., method="rpart", data = training)
print(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
set.seed(125)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.7, list = F)
data(segmentationOriginal)
set.seed(125)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.7, list = F)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
modFit <- train(Case ~., method="rpart", data = training)
print(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
set.seed(125)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.7, list = F)
training <- segmentationOriginal[inTrain,]
data(segmentationOriginal)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.7, list = F)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
modFit <- train(Case ~., method="rpart", data = training)
fancyRpartPlot(modFit$finalModel)
set.seed(125)
modFit <- train(Case ~., method="rpart", data = training)
fancyRpartPlot(modFit$finalModel)
data(segmentationOriginal)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.6, list = F)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
set.seed(125)
modFit <- train(Case ~., method="rpart", data = training)
fancyRpartPlot(modFit$finalModel)
set.seed(125)
modFit <- train(Class ~., method="rpart", data = training)
fancyRpartPlot(modFit$finalModel)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.7, list = F)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Class ~., method="rpart", data = training)
print(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
View(olive)
newdata = as.data.frame(t(colMeans(olive)))
View(newdata)
modFit <- train(Area ~., method = "rpart", data = olive)
predict(modFit, newdata = newdata)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
head(trainSA)
modellm <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm",
family = "binomial", data = trainSA)
str(trainSA)
str(SAheart)
trainSA$chd <- as.factor(trainSA$chd)
modellm <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm",
family = "binomial", data = trainSA)
testSA$chd <- as.factor(testSA$chd)
testSA$prediction <- predict(modellm, newdata = testSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, testSA$prediction)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, testSA$prediction)
modellm$pred
data(SAheart)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
modellm <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm",
family = "binomial", data = trainSA)
testSA$prediction <- predict(modellm, newdata = testSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, testSA$prediction)
trainSA$prediction <- predict(modellm, newdata = trainSA)
missClass(testSA$chd, testSA$prediction)
missClass(trainSA$chd, trainSA$prediction)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
modellm <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm",
family = "binomial", data = trainSA)
testSA$prediction <- predict(modellm, newdata = testSA)
trainSA$prediction <- predict(modellm, newdata = trainSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, testSA$prediction)
missClass(trainSA$chd, trainSA$prediction)
data(vowel.train)
data(vowel.test)
head(vowel.train)
vowel.train$y <- as.factor(vowel.train)
vowel.train$y <- as.factor(vowel.train$y)
str(vowel.train)
vowel.test$y <- as.factor(vowel.test$y)
modelrf <- train(y ~., method = "rf", data = vowel.train, prox = T)
modelrf
varlmp(modelrf)
varImp(modelrf)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
modelrf <- train(y~., data = vowel.train, method = "rf")
modelboost <- train(y~., data = vowel.train, method = "gbm")
modelboost <- train(y~., data = vowel.train, method = "gbm", verbose = F)
vowel.test$predictrf <- predict(modelrf, newdata = vowel.test)
vowel.test$predictboost <- predict(modelboost, newdata = vowel.test)
preddf <- data.frame(modelrf = vowel.test$predictrf, modelboost = vowel.test$predictboost, y = vowel.test$y)
combmodfit <- train(y~., method = "gam", data = preddf)
preddf$predcomb <- predict(combmodfit,preddf)
View(preddf)
confusionMatrix(preddf$modelrf, vowel.test$y)
confusionMatrix(preddf$modelboost, vowel.test$y)
confusionMatrix(preddf$predcomb, vowel.test$y)
confusionMatrix(preddf$predcomb, preddf$y)
library("AppliedPredictiveModeling", lib.loc="~/R/win-library/3.4")
set.seed(3433)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(adData)
modelrf <- train(diagnosis~,. data = training, method = "rf")
modelrf <- train(diagnosis~., data = training, method = "rf")
modelgbm <- train(diagnosis~., data = training, method = "gbm", verbose = F)
modellda <- train(diagnosis~., data = training, method = "lda")
preddf <- data.frame(predrf = predict(modelrf, newdata = testing), predgbm = predict(modelgbm, newdata = testing),
predlda = predict(modellda, newdata = testing), diagnosis = testing$diagnosis)
View(preddf)
combmodfit <- train(diagnosis ~ ., method = "rf", data = preddf)
combpred <- predict(combmodfit, preddf)
confusionMatrix(combpred, preddf$diagnosis)
confusionMatrix(preddf$predrf, preddf$diagnosis)
confusionMatrix(preddf$predgbm, preddf$diagnosis)
confusionMatrix(preddf$predlda, preddf$diagnosis)
confusionMatrix(preddf$predrf, preddf$diagnosis)$overall[1]
confusionMatrix(preddf$predgbm, preddf$diagnosis)$overall[1]
confusionMatrix(preddf$predlda, preddf$diagnosis)$overall[1]
confusionMatrix(combpred, preddf$diagnosis)$overall[1]
set.seed(3523)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
View(training)
?plot.enet
??plot.enet
modellasso <- train(CompressiveStrength~., method = "lasso", data = training)
?plot.enet
install.packages("lars")
plot.enet(modellasso$finalModel, xvar = "penalty")
?plot.enet
library("elasticnet", lib.loc="~/R/win-library/3.4")
plot.enet(modellasso$finalModel, xvar = "penalty")
library(lubridate) # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
setwd("~/Your Professionals/Coursera/Data Science R/7. Machine Learning/Quiz")
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library("forecast", lib.loc="~/R/win-library/3.4")
?bats
bats(tstrain)
model <- bats(tstrain)
fcast <- forecast(model)
accuracy(fcast, tstrain)
plot(fcast)
plot(tstrain)
plot(fcast)
plot(fcast)
lines(testing, col = "red")
accuracy(fcast, testing)
accuracy(fcast, testing)
model <- bats(training$visitsTumblr)
fcast <- forecast(model, level = 95, h = dim(testing)[1])
plot(fcast)
accuracy(fcast, testing)
sum(fcast$lower < testing$visitsTumblr &  testing$visitsTumblr < fcast$upper)/nrow(testing)
set.seed(3523)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library("AppliedPredictiveModeling", lib.loc="~/R/win-library/3.4")
set.seed(3523)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library("caret", lib.loc="~/R/win-library/3.4")
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library("e1071", lib.loc="~/R/win-library/3.4")
modelsvm <- train(CompressiveStrength, method = "svmLinearWeights", data = training)
View(training)
modelsvm <- train(CompressiveStrength~., method = "svmLinearWeights", data = training)
modelsvm <- train(CompressiveStrength~., method = "svmLinear2", data = training)
set.seed(325)
modelsvm <- train(CompressiveStrength~., method = "svmLinear2", data = training)
predict <- predict(modelsvm, newdata = testing)
error <- testing$CompressiveStrength - predict
rmse <- function(error)
{
sqrt(mean(error^2))
}
rmse(error)
accuracy(predict, testing$CompressiveStrength)
svm <- svm(CompressiveStrength ~ ., data = training)
pred <- predict(svm, newdata = testing)
accuracy(pred, testing$CompressiveStrength)
setwd("~/Your Professionals/Coursera/Data Science R/7. Machine Learning/Course Project")
training <- read.csv("pml-training.csv")
head(training)
?read.csv
head(training)
training <- read.csv("pml-training.csv", header = T, sep = ",", dec = "."  )
training <- read.csv("pml-training.csv", header = T, sep = ",", dec = ".", na.strings = "")
head(training)
testing <- read.csv("pml-testing.csv", header = T, sep = ",", dec = ".", na.strings = "")
head(testing)
View(training)
# We look at the structure of the data
str(training)
# training data
training <- read.csv("pml-training.csv", header = T, sep = ",", dec = ".", na.strings = c("", "#DIV/0!"))
training <- training[, -c("X", "user_name", "raw_timestamp_part1", "raw_timestamp_part2", "cvtd_timestamp", "new_window", "nuw_window")]
training[c("X", "user_name", "raw_timestamp_part1", "raw_timestamp_part2", "cvtd_timestamp", "new_window", "nuw_window")] <- list(NULL)
# We look at the structure of the data
str(training)
training[c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")] <- list(NULL)
# remove useless columns in training
training[c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")] <- list(NULL)
# remove useless columns in testing set
training[c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")] <- list(NULL)
# remove useless columns in testing set
testing[c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")] <- list(NULL)
View(testing)
missing <- c()
for(i in 1:ncol(training)) {
if(length(which(is.na(training[,i]))) > 0.5*nrow(training)) missing <- append(missing,i)
}
# remove the columns from dataset
training <- training[-missing,]
# training data
training <- read.csv("pml-training.csv", header = T, sep = ",", dec = ".", na.strings = c("", "#DIV/0!"))
# testing data
testing <- read.csv("pml-testing.csv", header = T, sep = ",", dec = ".", na.strings = c("", "#DIV/0!"))
# remove useless columns in training
training[c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")] <- list(NULL)
# remove useless columns in testing set
testing[c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")] <- list(NULL)
# find the column number with more than 50% missing
missing <- c()
for(i in 1:ncol(training)) {
if(length(which(is.na(training[,i]))) > 0.5*nrow(training)) missing <- append(missing,i)
}
# remove the columns from dataset
training <- training[,-missing]
testing <- testing[,-missing]
zero <- nearZeroVar(training)
zero <- nearZeroVar(training)
training <- training[,zero]
testing <- testing[,zero]
View(training)
# training data
training <- read.csv("pml-training.csv", header = T, sep = ",", dec = ".", na.strings = c("", "#DIV/0!"))
# testing data
testing <- read.csv("pml-testing.csv", header = T, sep = ",", dec = ".", na.strings = c("", "#DIV/0!"))
# remove useless columns in training
training[c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")] <- list(NULL)
# remove useless columns in testing set
testing[c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")] <- list(NULL)
# find the column number with more than 50% missing
missing <- c()
for(i in 1:ncol(training)) {
if(length(which(is.na(training[,i]))) > 0.5*nrow(training)) missing <- append(missing,i)
}
# training data
training <- read.csv("pml-training.csv", header = T, sep = ",", dec = ".", na.strings = c("", "#DIV/0!", "NA"))
# testing data
testing <- read.csv("pml-testing.csv", header = T, sep = ",", dec = ".", na.strings = c("", "#DIV/0!", "NA"))
# find the column number with more than 50% missing
missing <- c()
for(i in 1:ncol(training)) {
if(length(which(is.na(training[,i]))) > 0.5*nrow(training)) missing <- append(missing,i)
}
# remove useless columns in training
training[c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")] <- list(NULL)
# remove useless columns in testing set
testing[c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")] <- list(NULL)
# find the column number with more than 50% missing
missing <- c()
for(i in 1:ncol(training)) {
if(length(which(is.na(training[,i]))) > 0.5*nrow(training)) missing <- append(missing,i)
}
# remove the columns from dataset
training <- training[,-missing]
testing <- testing[,-missing]
# find the columns with near zero variance and remove them from the datasets
zero <- nearZeroVar(training)
# find the columns with near zero variance and remove them from the datasets
zero <- nearZeroVar(training)
training <- training[,-zero]
testing <- testing[,-zero]
# training data
training <- read.csv("pml-training.csv", header = T, sep = ",", dec = ".", na.strings = c("", "#DIV/0!", "NA"))
# testing data
testing <- read.csv("pml-testing.csv", header = T, sep = ",", dec = ".", na.strings = c("", "#DIV/0!", "NA"))
# remove useless columns in training
training[c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")] <- list(NULL)
# remove useless columns in testing set
testing[c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")] <- list(NULL)
# remove the columns from dataset
training <- training[,-missing]
testing <- testing[,-missing]
# find the columns with near zero variance and remove them from the datasets
zero <- nearZeroVar(training)
# find the columns with near zero variance and remove them from the datasets
zero <- nearZeroVar(training, saveMetrics = T)
View(zero)
zero
inTrain <- createDataPartition(y=training, p=0.7, list=FALSE)
inTrain <- createDataPartition(y=training$classe, p=0.7, list=FALSE)
ttraining <- training[inTrain, ]
ttesting <- training[-inTrain, ]
dim(newTraining); dim(newTesting)
dim(ttraining)
dim(ttesting)
set.seed(1234)
modelRF <- train(classe~., data = ttraining, method = "rf", prox = T)
